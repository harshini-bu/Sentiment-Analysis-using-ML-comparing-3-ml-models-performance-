import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import re
from nltk.sentiment import SentimentIntensityAnalyzer
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score

# Download NLTK resources (if not already downloaded)
nltk.download('punkt')  # for tokenization
nltk.download('stopwords')  # for stopwords
nltk.download('wordnet')  # for lemmatization
nltk.download('vader_lexicon')  # for sentiment analysis lexicon

# Load the training and test datasets with specified encoding
train_df = pd.read_csv("train.csv", encoding='latin1')
test_df = pd.read_csv("test.csv", encoding='latin1')

# Keep only 'text' and 'sentiment' features
train_df = train_df[['text', 'sentiment']]
test_df = test_df[['text', 'sentiment']]

# Check for missing values
print("Missing values in train dataset:")
print(train_df.isnull().sum())
print("\nMissing values in test dataset:")
print(test_df.isnull().sum())

# Drop rows with missing values
train_df = train_df.dropna(subset=['text'])
test_df = test_df.dropna(subset=['sentiment'])
test_df = test_df.dropna(subset=['text'])

print("Missing values in train dataset:")
print(train_df.isnull().sum())
print("\nMissing values in test dataset:")
print(test_df.isnull().sum())

# Drop rows with sentiment as neutral from train dataset
train_df = train_df[train_df['sentiment'] != 'neutral']

# Drop rows with sentiment as neutral from test dataset
test_df = test_df[test_df['sentiment'] != 'neutral']

plt.hist(train_df['sentiment'])
plt.show()
plt.hist(test_df['sentiment'])
plt.show()

from sklearn.utils import resample

# Separate the train dataset into classes
negative_df = train_df[train_df['sentiment'] == 'negative']
positive_df = train_df[train_df['sentiment'] == 'positive']

# Downsample the majority class to the size of the minority classes
n_samples = min(len(negative_df), len(positive_df))

negative_downsampled = resample(negative_df, replace=False, n_samples=n_samples, random_state=42)
positive_downsampled = resample(positive_df, replace=False, n_samples=n_samples, random_state=42)

# Concatenate the downsampled dataframes
train_downsampled = pd.concat([negative_downsampled, positive_downsampled])

# Shuffle the downsampled dataframe
train_downsampled = train_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)

# Check the class distribution
print(train_downsampled['sentiment'].value_counts())

train_downsampled.to_csv("svm_data.csv", index=False)
train_df= pd.read_csv("svm_data.csv", encoding='latin1')

# Tokenization
train_df['text_tokens'] = train_df['text'].apply(lambda x: word_tokenize(x.lower()))
test_df['text_tokens'] = test_df['text'].apply(lambda x: word_tokenize(x.lower()))

# Text Cleaning
train_df['clean_text'] = train_df['text_tokens'].apply(lambda tokens: [re.sub(r'[^a-zA-Z\s]', '', token) for token in tokens if re.sub(r'[^a-zA-Z\s]', '', token) != ''])
test_df['clean_text'] = test_df['text_tokens'].apply(lambda tokens: [re.sub(r'[^a-zA-Z\s]', '', token) for token in tokens if re.sub(r'[^a-zA-Z\s]', '', token) != ''])

# Stopword Removal
stop_words = set(stopwords.words('english'))
train_df['clean_text'] = train_df['clean_text'].apply(lambda tokens: [token for token in tokens if token not in stop_words])
test_df['clean_text'] = test_df['clean_text'].apply(lambda tokens: [token for token in tokens if token not in stop_words])

# Lemmatization
lemmatizer = WordNetLemmatizer()
train_df['lemmatized_text'] = train_df['clean_text'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])
test_df['lemmatized_text'] = test_df['clean_text'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])

# Vectorization (TF-IDF)
tfidf_vectorizer = TfidfVectorizer()
train_tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['lemmatized_text'].apply(lambda x: ' '.join(x)))
test_tfidf_matrix = tfidf_vectorizer.transform(test_df['lemmatized_text'].apply(lambda x: ' '.join(x)))

# Sentiment Lexicons
sid = SentimentIntensityAnalyzer()
train_df['positive_words'] = train_df['text'].apply(lambda x: len([word for word in x.split() if sid.polarity_scores(word)['compound'] > 0]))
train_df['negative_words'] = train_df['text'].apply(lambda x: len([word for word in x.split() if sid.polarity_scores(word)['compound'] < 0]))

test_df['positive_words'] = test_df['text'].apply(lambda x: len([word for word in x.split() if sid.polarity_scores(word)['compound'] > 0]))
test_df['negative_words'] = test_df['text'].apply(lambda x: len([word for word in x.split() if sid.polarity_scores(word)['compound'] < 0]))


from sklearn.svm import SVC

# Initialize Support Vector Machine (SVM) model
svm_model = SVC()

# Perform cross-validation
cv_scores = cross_val_score(svm_model, train_tfidf_matrix, train_df['sentiment'], cv=5)

# Print cross-validation scores
print("Cross-validation Scores:", cv_scores)
print("Mean Accuracy:", cv_scores.mean())

# Fit the model on TF-IDF features
svm_model.fit(train_tfidf_matrix, train_df['sentiment'])

# Predict on the test set
predictions = svm_model.predict(test_tfidf_matrix)

# Evaluate accuracy and classification report
accuracy = accuracy_score(test_df['sentiment'], predictions)
classification_rep = classification_report(test_df['sentiment'], predictions)

print("\nAccuracy:", accuracy)
print("\nClassification Report:")
print(classification_rep)
