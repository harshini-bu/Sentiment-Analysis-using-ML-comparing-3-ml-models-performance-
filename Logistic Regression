import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import re
from nltk.sentiment import SentimentIntensityAnalyzer
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, classification_report


from sklearn.linear_model import LogisticRegression
# Download NLTK resources (if not already downloaded)
nltk.download('punkt')  # for tokenization
nltk.download('stopwords')  # for stopwords
nltk.download('wordnet')  # for lemmatization
nltk.download('vader_lexicon')  # for sentiment analysis lexicon

# Load the training and test datasets with specified encoding
train_df = pd.read_csv("train.csv", encoding='latin1')
test_df = pd.read_csv("test.csv", encoding='latin1')

# Keep only 'text' and 'sentiment' features
train_df = train_df[['text', 'sentiment']]
test_df = test_df[['text', 'sentiment']]

# Check for missing values
print("Missing values in train dataset:")
print(train_df.isnull().sum())
print("\nMissing values in test dataset:")
print(test_df.isnull().sum())

# Drop rows with missing values
train_df = train_df.dropna(subset=['text'])
test_df = test_df.dropna(subset=['sentiment'])
test_df = test_df.dropna(subset=['text'])

print("Missing values in train dataset:")
print(train_df.isnull().sum())
print("\nMissing values in test dataset:")
print(test_df.isnull().sum())

# Drop rows with sentiment as neutral from train dataset
train_df = train_df[train_df['sentiment'] != 'neutral']

# Drop rows with sentiment as neutral from test dataset
test_df = test_df[test_df['sentiment'] != 'neutral']

plt.hist(train_df['sentiment'])
plt.show()
#plt.hist(test_df['sentiment'])
#plt.show()

from sklearn.utils import resample

# Separate the train dataset into classes
negative_df = train_df[train_df['sentiment'] == 'negative']
positive_df = train_df[train_df['sentiment'] == 'positive']

# Downsample the majority class to the size of the minority classes
n_samples = min(len(negative_df), len(positive_df))

negative_downsampled = resample(negative_df, replace=False, n_samples=n_samples, random_state=42)
positive_downsampled = resample(positive_df, replace=False, n_samples=n_samples, random_state=42)

# Concatenate the downsampled dataframes
train_downsampled = pd.concat([negative_downsampled, positive_downsampled])

# Shuffle the downsampled dataframe
train_downsampled = train_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)


train_downsampled.to_csv("clean_train_downsampled_data.csv", index=False)
train_df= pd.read_csv("clean_train_downsampled_data.csv", encoding='latin1')



# Tokenization
train_df['text_tokens'] = train_df['text'].apply(lambda x: word_tokenize(x.lower()))
test_df['text_tokens'] = test_df['text'].apply(lambda x: word_tokenize(x.lower()))

# Text Cleaning
train_df['clean_text'] = train_df['text_tokens'].apply(lambda tokens: [re.sub(r'[^a-zA-Z\s]', '', token) for token in tokens if re.sub(r'[^a-zA-Z\s]', '', token) != ''])
test_df['clean_text'] = test_df['text_tokens'].apply(lambda tokens: [re.sub(r'[^a-zA-Z\s]', '', token) for token in tokens if re.sub(r'[^a-zA-Z\s]', '', token) != ''])

# Stopword Removal
stop_words = set(stopwords.words('english'))
train_df['clean_text'] = train_df['clean_text'].apply(lambda tokens: [token for token in tokens if token not in stop_words])
test_df['clean_text'] = test_df['clean_text'].apply(lambda tokens: [token for token in tokens if token not in stop_words])

# Lemmatization
lemmatizer = WordNetLemmatizer()
train_df['lemmatized_text'] = train_df['clean_text'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])
test_df['lemmatized_text'] = test_df['clean_text'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])

# Vectorization (TF-IDF)
tfidf_vectorizer = TfidfVectorizer()
train_tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['lemmatized_text'].apply(lambda x: ' '.join(x)))
test_tfidf_matrix = tfidf_vectorizer.transform(test_df['lemmatized_text'].apply(lambda x: ' '.join(x)))

# Sentiment Lexicons
sid = SentimentIntensityAnalyzer()
train_df['positive_words'] = train_df['text'].apply(lambda x: len([word for word in x.split() if sid.polarity_scores(word)['compound'] > 0]))
train_df['negative_words'] = train_df['text'].apply(lambda x: len([word for word in x.split() if sid.polarity_scores(word)['compound'] < 0]))

test_df['positive_words'] = test_df['text'].apply(lambda x: len([word for word in x.split() if sid.polarity_scores(word)['compound'] > 0]))
test_df['negative_words'] = test_df['text'].apply(lambda x: len([word for word in x.split() if sid.polarity_scores(word)['compound'] < 0]))
print(train_df.columns)




# Initialize Logistic Regression modelnn 
logistic_model = LogisticRegression()

# Fit the model on TF-IDF features with cross-validation
cv_scores = cross_val_score(logistic_model, train_tfidf_matrix, train_df['sentiment'], cv=5)

# Print cross-validation scores
print("Cross-validation Scores:", cv_scores)
print("Mean Accuracy:", cv_scores.mean())

# Predict on the test set
logistic_model.fit(train_tfidf_matrix, train_df['sentiment'])


predictions = logistic_model.predict(test_tfidf_matrix)

# Evaluate accuracy and classification report
accuracy = accuracy_score(test_df['sentiment'], predictions)
classification_rep = classification_report(test_df['sentiment'], predictions)

print("\nAccuracy:", accuracy)
print("\nClassification Report:\n")
print(classification_rep)
